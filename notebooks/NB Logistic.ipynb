{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, ParameterSampler\n",
    "from sklearn.metrics import accuracy_score, log_loss, precision_score, recall_score\n",
    "\n",
    "from scipy.stats.distributions import uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LABEL_COLUMNS = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../dataset/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = pd.concat([train_data.comment_text.astype(str), \n",
    "                       test_data.comment_text.astype(str)], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(ngram_range=(1,2), min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1, max_features=50000)\n",
    "vec.fit(documents)\n",
    "x_train = vec.transform(train_data.comment_text.astype(str))\n",
    "x_test = vec.transform(test_data.comment_text.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_copy = x_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pr(x, y_i, y):\n",
    "    \n",
    "    p = x[y==y_i].sum(0)\n",
    "    \n",
    "    return (p+1) / ((y==y_i).sum()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mdl(x, y, c=4, dual=True):\n",
    "    y = y.values\n",
    "    r = np.log(pr(x, 1,y) / pr(x, 0,y))\n",
    "    \n",
    "    m = LogisticRegression(C=c, dual=dual)\n",
    "    \n",
    "    x_nb = x.multiply(r)\n",
    "    \n",
    "    return m.fit(x_nb, y), r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metrics = ['log_loss', 'accuracy', 'recall', 'precision']\n",
    "\n",
    "def average(scores):\n",
    "    length = len(scores) if len(scores) > 0 else 1\n",
    "    \n",
    "    return float(sum(scores)) / length\n",
    "\n",
    "def get_label(probs):\n",
    "    probs[probs >= .5 ] = 1\n",
    "    probs[probs < .5] = 0\n",
    "    \n",
    "    return probs\n",
    "\n",
    "\n",
    "def run_kfold(x, y):\n",
    "    k_fold = KFold(n_splits=5, random_state=1234)\n",
    "    \n",
    "    fold_scores = {label_column: {metric_name: [] for metric_name in metrics}\n",
    "                   for label_column in LABEL_COLUMNS}\n",
    "    best_log_loss = {label_column: 1 for label_column in LABEL_COLUMNS}\n",
    "    best_params = {label_column: None for label_column in LABEL_COLUMNS}\n",
    "    \n",
    "    param_grid = {'c': uniform(1, 5)}\n",
    "    param_sampler = ParameterSampler(param_grid, n_iter=5)\n",
    "    \n",
    "    for i, label_column in enumerate(LABEL_COLUMNS):\n",
    "        for params in param_sampler:\n",
    "            for fold_i, (train_index, test_index) in enumerate(k_fold.split(x)):\n",
    "    #             print('Fitting models on fold: {}'.format(fold_i + 1))\n",
    "\n",
    "                x_train_split, y_train_split = x[train_index, :], y[label_column].iloc[train_index]\n",
    "                x_test_split, y_test_split = x[test_index, :], y[label_column].iloc[test_index]\n",
    "\n",
    "                m,r = get_mdl(x_train_split, y_train_split, c=params['c'])\n",
    "\n",
    "                preds = m.predict_proba(x_test_split.multiply(r))[:,1]\n",
    "\n",
    "                log_loss_score = log_loss(y_test_split, preds)\n",
    "\n",
    "                pred_labels = get_label(preds)\n",
    "\n",
    "                accuracy = accuracy_score(y_test_split, pred_labels)\n",
    "                recall = recall_score(y_test_split, pred_labels)\n",
    "                precision = precision_score(y_test_split, pred_labels)\n",
    "\n",
    "                fold_scores[label_column]['log_loss'].append(log_loss_score)\n",
    "                fold_scores[label_column]['accuracy'].append(accuracy)\n",
    "                fold_scores[label_column]['recall'].append(recall)\n",
    "                fold_scores[label_column]['precision'].append(precision)\n",
    "\n",
    "            fold_avgs = {label_column: {score_name: average(fold_scores[label_column][score_name]) \n",
    "                         for score_name in metrics} for label_column in LABEL_COLUMNS}\n",
    "\n",
    "    #         for label_column, scores in fold_avgs.items():\n",
    "    #             print('-----------------------------------------')\n",
    "    #             print(label_column)\n",
    "\n",
    "            for score_name, score_average in fold_avgs[label_column].items():\n",
    "                print('{} average: {}'.format(score_name, score_average))\n",
    "            print('-----------------------------------------\\n')\n",
    "\n",
    "            total_avg_log_loss = average(list(map(lambda scores: scores['log_loss'], map(lambda column_name: fold_avgs[column_name], \n",
    "                                                                            LABEL_COLUMNS))))\n",
    "            \n",
    "            if fold_avgs[label_column]['log_loss'] < best_log_loss[label_column]:\n",
    "                best_log_loss[label_column] = fold_avgs[label_column]['log_loss']\n",
    "                best_params[label_column] = params\n",
    "                \n",
    "            print('Average log loss on all labels: {}'.format(total_avg_log_loss))\n",
    "    return best_log_loss, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall average: 0.6961960601151314\n",
      "accuracy average: 0.9625558550346185\n",
      "log_loss average: 0.10316248724039172\n",
      "precision average: 0.8890712355528873\n",
      "-----------------------------------------\n",
      "\n",
      "Average log loss on all labels: 0.01719374787339862\n",
      "recall average: 0.6996647814481565\n",
      "accuracy average: 0.962502587760065\n",
      "log_loss average: 0.10345260323848801\n",
      "precision average: 0.8850795106808181\n",
      "-----------------------------------------\n",
      "\n",
      "Average log loss on all labels: 0.017242100539748003\n",
      "recall average: 0.7020792460811924\n",
      "accuracy average: 0.9624284305535433\n",
      "log_loss average: 0.10395688109687824\n",
      "precision average: 0.8819127629502257\n",
      "-----------------------------------------\n",
      "\n",
      "Average log loss on all labels: 0.017326146849479706\n",
      "recall average: 0.7015408940595314\n",
      "accuracy average: 0.9624790872002269\n",
      "log_loss average: 0.10377397996173907\n",
      "precision average: 0.8830057730138773\n",
      "-----------------------------------------\n",
      "\n",
      "Average log loss on all labels: 0.01729566332695651\n",
      "recall average: 0.702601135913378\n",
      "accuracy average: 0.9624430530935645\n",
      "log_loss average: 0.103999065899278\n",
      "precision average: 0.8815759571871351\n",
      "-----------------------------------------\n",
      "\n",
      "Average log loss on all labels: 0.017333177649879666\n",
      "recall average: 0.2858882878624766\n",
      "accuracy average: 0.989941782185244\n",
      "log_loss average: 0.03183130447456602\n",
      "precision average: 0.4959267307578653\n",
      "-----------------------------------------\n",
      "\n",
      "Average log loss on all labels: 0.022638395062307335\n",
      "recall average: 0.28443840370348983\n",
      "accuracy average: 0.9899668498483377\n",
      "log_loss average: 0.030858058865860145\n",
      "precision average: 0.49807540596973227\n",
      "-----------------------------------------\n",
      "\n",
      "Average log loss on all labels: 0.022476187460856358\n",
      "recall average: 0.28514281703714023\n",
      "accuracy average: 0.9899292489591419\n",
      "log_loss average: 0.03169596418527541\n",
      "precision average: 0.49482617636374215\n",
      "-----------------------------------------\n",
      "\n",
      "Average log loss on all labels: 0.02261583834742557\n",
      "recall average: 0.2854947476573816\n",
      "accuracy average: 0.9899402158183402\n",
      "log_loss average: 0.03167908766377288\n",
      "precision average: 0.4958039132748798\n",
      "-----------------------------------------\n",
      "\n",
      "Average log loss on all labels: 0.022613025593841814\n",
      "recall average: 0.28511274363798306\n",
      "accuracy average: 0.9899818901320171\n",
      "log_loss average: 0.03120620819231228\n",
      "precision average: 0.4995302450029662\n",
      "-----------------------------------------\n",
      "\n",
      "Average log loss on all labels: 0.02253421234859838\n",
      "recall average: 0.7233893809872763\n",
      "accuracy average: 0.979889830034464\n",
      "log_loss average: 0.06162917557496441\n",
      "precision average: 0.8752041890065844\n",
      "-----------------------------------------\n",
      "\n",
      "Average log loss on all labels: 0.03280574161109245\n",
      "recall average: 0.7243308652729283\n",
      "accuracy average: 0.9800151657972467\n",
      "log_loss average: 0.060967876947727616\n",
      "precision average: 0.8768352754434166\n",
      "-----------------------------------------\n",
      "\n",
      "Average log loss on all labels: 0.03269552517321965\n",
      "recall average: 0.7240569812560015\n",
      "accuracy average: 0.97997129839318\n",
      "log_loss average: 0.06123388360806362\n",
      "precision average: 0.8762180059349955\n",
      "-----------------------------------------\n",
      "\n",
      "Average log loss on all labels: 0.03273985961660898\n",
      "recall average: 0.7243292279035556\n",
      "accuracy average: 0.9800386666516255\n",
      "log_loss average: 0.06082547133775967\n",
      "precision average: 0.8773122507733919\n",
      "-----------------------------------------\n",
      "\n",
      "Average log loss on all labels: 0.03267179090489166\n",
      "recall average: 0.7233328096633592\n",
      "accuracy average: 0.9801104210712146\n",
      "log_loss average: 0.060267795123600774\n",
      "precision average: 0.8797009091679038\n",
      "-----------------------------------------\n",
      "\n",
      "Average log loss on all labels: 0.03257884486919851\n",
      "recall average: 0.29142477509009357\n",
      "accuracy average: 0.9974368780597308\n",
      "log_loss average: 0.009637744069563808\n",
      "precision average: 0.666743523912972\n",
      "-----------------------------------------\n",
      "\n",
      "Average log loss on all labels: 0.034185135547459146\n",
      "recall average: 0.29503450827094235\n",
      "accuracy average: 0.9974149444067877\n",
      "log_loss average: 0.010103370433389939\n",
      "precision average: 0.653611533730152\n",
      "-----------------------------------------\n",
      "\n",
      "Average log loss on all labels: 0.03426273994143016\n",
      "recall average: 0.2955131149833992\n",
      "accuracy average: 0.9974076331891397\n",
      "log_loss average: 0.010203806611649966\n",
      "precision average: 0.6496165425128713\n",
      "-----------------------------------------\n",
      "\n",
      "Average log loss on all labels: 0.034279479304473504\n",
      "recall average: 0.29623318757039685\n",
      "accuracy average: 0.9974086776137394\n",
      "log_loss average: 0.010147996622045106\n",
      "precision average: 0.6497268660185112\n",
      "-----------------------------------------\n",
      "\n",
      "Average log loss on all labels: 0.03427017763953936\n",
      "recall average: 0.29634611591706295\n",
      "accuracy average: 0.9974105576369268\n",
      "log_loss average: 0.010087170140387508\n",
      "precision average: 0.6505581730451477\n",
      "-----------------------------------------\n",
      "\n",
      "Average log loss on all labels: 0.034260039892596426\n",
      "recall average: 0.6037578213283171\n",
      "accuracy average: 0.9730339516653576\n",
      "log_loss average: 0.07579103747654867\n",
      "precision average: 0.801353157532178\n",
      "-----------------------------------------\n",
      "\n",
      "Average log loss on all labels: 0.046891879472021204\n",
      "recall average: 0.6050124583059905\n",
      "accuracy average: 0.9726861438902891\n",
      "log_loss average: 0.07792124383083593\n",
      "precision average: 0.7930646045176342\n",
      "-----------------------------------------\n",
      "\n",
      "Average log loss on all labels: 0.04724691386440242\n",
      "recall average: 0.6053882914991545\n",
      "accuracy average: 0.9725785636893304\n",
      "log_loss average: 0.07853399792691415\n",
      "precision average: 0.7905088227497641\n",
      "-----------------------------------------\n",
      "\n",
      "Average log loss on all labels: 0.04734903954708212\n",
      "recall average: 0.6058339813061154\n",
      "accuracy average: 0.9725561076031818\n",
      "log_loss average: 0.07875550737569031\n",
      "precision average: 0.7896889267065897\n",
      "-----------------------------------------\n",
      "\n",
      "Average log loss on all labels: 0.04738595778854482\n",
      "recall average: 0.6052938390735013\n",
      "accuracy average: 0.9726541831917445\n",
      "log_loss average: 0.07814396269339434\n",
      "precision average: 0.792178082846252\n",
      "-----------------------------------------\n",
      "\n",
      "Average log loss on all labels: 0.047284033674828814\n",
      "recall average: 0.29720229433273576\n",
      "accuracy average: 0.9921226302895674\n",
      "log_loss average: 0.02830042486793184\n",
      "precision average: 0.6100479375267394\n",
      "-----------------------------------------\n",
      "\n",
      "Average log loss on all labels: 0.052000771152817454\n",
      "recall average: 0.29404239731166665\n",
      "accuracy average: 0.9921664975954541\n",
      "log_loss average: 0.027769701962268196\n",
      "precision average: 0.6179654516191382\n",
      "-----------------------------------------\n",
      "\n",
      "Average log loss on all labels: 0.05191231733520685\n",
      "recall average: 0.2949163359816396\n",
      "accuracy average: 0.9921226301586604\n",
      "log_loss average: 0.028203990905159237\n",
      "precision average: 0.611300411431492\n",
      "-----------------------------------------\n",
      "\n",
      "Average log loss on all labels: 0.05198469882568869\n",
      "recall average: 0.2899824202213735\n",
      "accuracy average: 0.9921570974304267\n",
      "log_loss average: 0.02776880910976471\n",
      "precision average: 0.619401566953559\n",
      "-----------------------------------------\n",
      "\n",
      "Average log loss on all labels: 0.05191216852645627\n",
      "recall average: 0.29172722867613454\n",
      "accuracy average: 0.9921238834616345\n",
      "log_loss average: 0.02814654620681566\n",
      "precision average: 0.613720872790182\n",
      "-----------------------------------------\n",
      "\n",
      "Average log loss on all labels: 0.0519751247092981\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'identity_hate': 0.02776880910976471,\n",
       "  'insult': 0.07579103747654867,\n",
       "  'obscene': 0.060267795123600774,\n",
       "  'severe_toxic': 0.030858058865860145,\n",
       "  'threat': 0.009637744069563808,\n",
       "  'toxic': 0.10316248724039172},\n",
       " {'identity_hate': {'c': 1.130148539261989},\n",
       "  'insult': {'c': 2.4598680040526428},\n",
       "  'obscene': {'c': 1.3918340852560509},\n",
       "  'severe_toxic': {'c': 2.1667882795854196},\n",
       "  'threat': {'c': 1.0730969830764079},\n",
       "  'toxic': {'c': 2.3537381162760997}})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_kfold(x_train_copy, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.concat([test_data.id.to_frame(), pd.DataFrame(preds, columns = LABEL_COLUMNS, dtype=float)], axis=1)\n",
    "submission.to_csv('logistic_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_log_loss = average(list(map(lambda column_name: )))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
