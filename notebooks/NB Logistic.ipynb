{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, ParameterSampler\n",
    "from sklearn.metrics import accuracy_score, log_loss, precision_score, recall_score\n",
    "\n",
    "from scipy.stats.distributions import uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LABEL_COLUMNS = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../dataset/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../dataset/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = pd.concat([train_data.comment_text.astype(str), \n",
    "                       test_data.comment_text.astype(str)], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(ngram_range=(1,2), min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1, max_features=50000)\n",
    "vec.fit(documents)\n",
    "x_train = vec.transform(train_data.comment_text.astype(str))\n",
    "x_test = vec.transform(test_data.comment_text.astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_copy = x_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pr(x, y_i, y):\n",
    "    \n",
    "    p = x[y==y_i].sum(0)\n",
    "    \n",
    "    return (p+1) / ((y==y_i).sum()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mdl(x, y, c=4, dual=True):\n",
    "    y = y.values\n",
    "    r = np.log(pr(x, 1,y) / pr(x, 0,y))\n",
    "    \n",
    "    m = LogisticRegression(C=c, dual=dual)\n",
    "    \n",
    "    x_nb = x.multiply(r)\n",
    "    \n",
    "    return m.fit(x_nb, y), r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metrics = ['log_loss', 'accuracy', 'recall', 'precision']\n",
    "\n",
    "def average(scores):\n",
    "    length = len(scores) if len(scores) > 0 else 1\n",
    "    \n",
    "    return float(sum(scores)) / length\n",
    "\n",
    "def get_label(probs):\n",
    "    probs[probs >= .5 ] = 1\n",
    "    probs[probs < .5] = 0\n",
    "    \n",
    "    return probs\n",
    "\n",
    "\n",
    "def run_kfold(x, y):\n",
    "    k_fold = KFold(n_splits=5, random_state=1234)\n",
    "    \n",
    "    fold_scores = {label_column: {metric_name: [] for metric_name in metrics}\n",
    "                   for label_column in LABEL_COLUMNS}\n",
    "    best_log_loss = {label_column: 1 for label_column in LABEL_COLUMNS}\n",
    "    best_params = {label_column: None for label_column in LABEL_COLUMNS}\n",
    "    \n",
    "    param_grid = {'c': uniform(1.3, 2.4)}\n",
    "    param_sampler = ParameterSampler(param_grid, n_iter=10)\n",
    "    \n",
    "    for i, label_column in enumerate(LABEL_COLUMNS):\n",
    "        for params in param_sampler:\n",
    "            for fold_i, (train_index, test_index) in enumerate(k_fold.split(x)):\n",
    "    #             print('Fitting models on fold: {}'.format(fold_i + 1))\n",
    "\n",
    "                x_train_split, y_train_split = x[train_index, :], y[label_column].iloc[train_index]\n",
    "                x_test_split, y_test_split = x[test_index, :], y[label_column].iloc[test_index]\n",
    "\n",
    "                m,r = get_mdl(x_train_split, y_train_split, c=params['c'])\n",
    "\n",
    "                preds = m.predict_proba(x_test_split.multiply(r))[:,1]\n",
    "\n",
    "                log_loss_score = log_loss(y_test_split, preds)\n",
    "\n",
    "                pred_labels = get_label(preds)\n",
    "\n",
    "                accuracy = accuracy_score(y_test_split, pred_labels)\n",
    "                recall = recall_score(y_test_split, pred_labels)\n",
    "                precision = precision_score(y_test_split, pred_labels)\n",
    "\n",
    "                fold_scores[label_column]['log_loss'].append(log_loss_score)\n",
    "                fold_scores[label_column]['accuracy'].append(accuracy)\n",
    "                fold_scores[label_column]['recall'].append(recall)\n",
    "                fold_scores[label_column]['precision'].append(precision)\n",
    "\n",
    "            fold_avgs = {label_column: {score_name: average(fold_scores[label_column][score_name]) \n",
    "                         for score_name in metrics} for label_column in LABEL_COLUMNS}\n",
    "\n",
    "    #         for label_column, scores in fold_avgs.items():\n",
    "    #             print('-----------------------------------------')\n",
    "    #             print(label_column)\n",
    "\n",
    "#             for score_name, score_average in fold_avgs[label_column].items():\n",
    "#                 print('{} average: {}'.format(score_name, score_average))\n",
    "#             print('-----------------------------------------\\n')\n",
    "\n",
    "            total_avg_log_loss = average(list(map(lambda scores: scores['log_loss'], map(lambda column_name: fold_avgs[column_name], \n",
    "                                                                            LABEL_COLUMNS))))\n",
    "            \n",
    "            if fold_avgs[label_column]['log_loss'] < best_log_loss[label_column]:\n",
    "                best_log_loss[label_column] = fold_avgs[label_column]['log_loss']\n",
    "                best_params[label_column] = params\n",
    "                \n",
    "#             print('Average log loss on all labels: {}'.format(total_avg_log_loss))\n",
    "    return best_log_loss, best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_losses, c = run_kfold(x_train_copy, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_log_loss = average(list(map(lambda column_name: log_losses[column_name], LABEL_COLUMNS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05078621964141904"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'identity_hate': {'c': 1.7223574910194115},\n",
       " 'insult': {'c': 1.5465714849717698},\n",
       " 'obscene': {'c': 2.3246163376589211},\n",
       " 'severe_toxic': {'c': 1.3421517716983811},\n",
       " 'threat': {'c': 2.1944784949912961},\n",
       " 'toxic': {'c': 1.8996061003708009}}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.concat([test_data.id.to_frame(), pd.DataFrame(preds, columns = LABEL_COLUMNS, dtype=float)], axis=1)\n",
    "submission.to_csv('logistic_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
